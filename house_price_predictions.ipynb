{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Project Introduction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# (TITLE OF THE PROJECT)\n",
    "#### AIM - \n",
    "###### (LINKS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Core libraries\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# Visualization\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "# Date and time\n",
    "from datetime import datetime\n",
    "\n",
    "# Preprocessing and modeling\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.metrics import mean_absolute_error, mean_squared_error, r2_score\n",
    "from sklearn.linear_model import RidgeCV, LassoCV, ElasticNetCV\n",
    "import scipy.stats as stats\n",
    "from sklearn.tree import DecisionTreeRegressor\n",
    "from sklearn.model_selection import GridSearchCV, cross_val_score\n",
    "\n",
    "# Utilities\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Import"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# LOAD DATASETS\n",
    "domain = pd.read_csv(\"data/domain_properties.csv\")\n",
    "suburb = pd.read_csv(\"data/syd_sub_rev.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# CHECKING DATA\n",
    "print(\"Domain Properties\\n\")\n",
    "print(domain.info(), \"\\n\\n\", domain.head, \"\\n\\n\", domain.describe)\n",
    "print(\"\\n\\nSydney Suburb Review\\n\")\n",
    "print(suburb.info(), \"\\n\\n\", suburb.head, \"\\n\\n\", suburb.describe)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Basic Data Cleaning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# COLUMN STANDARDISATION\n",
    "domain.columns = domain.columns.str.strip().str.lower().str.replace(\" \", \"_\")\n",
    "suburb.columns = suburb.columns.str.strip().str.lower().str.replace(\" \", \"_\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# CLEANING SUBURB FIRST\n",
    "suburb.rename(columns={\"name\" : \"suburb\"}, inplace=True) # renaming the name column to suburb so we can merge datasets later\n",
    "\n",
    "# CHECK FOR DUPLICATE SUBURBS\n",
    "print(suburb[\"suburb\"].nunique(), \"unique suburbs out of\", suburb.shape[0], \"rows\")\n",
    "print(suburb[\"suburb\"].duplicated().sum(), \"duplicate suburb entries\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# REMOVING SYMBOLS AND CONVERTING TYPES\n",
    "def clean_currency(val):\n",
    "    if isinstance(val, str):\n",
    "        return pd.to_numeric(val.replace(\"$\", \"\").replace(\",\", \"\"), errors=\"coerce\")\n",
    "    return val\n",
    "\n",
    "def clean_percent(val):\n",
    "    if isinstance(val, str):\n",
    "        return pd.to_numeric(val.replace(\"%\", \"\"), errors=\"coerce\")\n",
    "    return val\n",
    "\n",
    "def clean_int(val):\n",
    "    if isinstance(val, str):\n",
    "        return pd.to_numeric(val.replace(\",\", \"\"), errors=\"coerce\")\n",
    "    return val\n",
    "\n",
    "suburb[\"population_(rounded)*\"] = suburb[\"population_(rounded)*\"].apply(clean_int)\n",
    "suburb[\"median_house_price_(2020)\"] = suburb[\"median_house_price_(2020)\"].apply(clean_currency)\n",
    "suburb[\"median_house_price_(2021)\"] = suburb[\"median_house_price_(2021)\"].apply(clean_currency)\n",
    "suburb[\"median_house_rent_(per_week)\"] = suburb[\"median_house_rent_(per_week)\"].apply(clean_currency)\n",
    "suburb[\"median_apartment_price_(2020)\"] = suburb[\"median_apartment_price_(2020)\"].apply(clean_currency)\n",
    "suburb[\"median_apartment_rent_(per_week)\"] = suburb[\"median_apartment_rent_(per_week)\"].apply(clean_currency)\n",
    "suburb[\"%_change\"] = suburb[\"%_change\"].apply(clean_percent)\n",
    "suburb[\"public_housing_%\"] = suburb[\"public_housing_%\"].apply(clean_percent)\n",
    "\n",
    "currency_cols = [\n",
    "    \"median_house_price_(2020)\", \"median_house_price_(2021)\",\n",
    "    \"median_apartment_price_(2020)\", \"median_house_rent_(per_week)\",\n",
    "    \"median_apartment_rent_(per_week)\"\n",
    "]\n",
    "\n",
    "percent_cols = [\"%_change\", \"public_housing_%\"]\n",
    "int_cols = [\"population_(rounded)*\"]\n",
    "\n",
    "for col in currency_cols:\n",
    "    suburb[col] = suburb[col].apply(clean_currency)\n",
    "\n",
    "for col in percent_cols:\n",
    "    suburb[col] = suburb[col].apply(clean_percent)\n",
    "\n",
    "for col in int_cols:\n",
    "    suburb[col] = suburb[col].apply(clean_int)\n",
    "\n",
    "suburb.dtypes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# DROPPING IRRELEVENT COLUMNS\n",
    "suburb.drop(columns=[\n",
    "    \"region\",\n",
    "    \"ethnic_breakdown_2016\",\n",
    "    \"nearest_train_station\",\n",
    "    \"highlights/attractions\",\n",
    "    \"ideal_for\",\n",
    "    \"review_link\"\n",
    "], inplace=True, errors=\"ignore\")\n",
    "\n",
    "# Extracting the numerical values from time and converting to float\n",
    "for col in [\"time_to_cbd_(public_transport)_[town_hall_st]\", \"time_to_cbd_(driving)_[town_hall_st]\"]:\n",
    "    suburb[col] = suburb[col].str.extract(r\"(\\d+)\").astype(float)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# CHECKING FOR NULLS IN SUBURB\n",
    "suburb.isnull().sum().sort_values(ascending=False), suburb.dtypes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# MAKING NULL NUMERIC VALS TO MEDIAN WHERE APPLICABLE\n",
    "median_cols = [\n",
    "    'median_apartment_price_(2020)',\n",
    "    'median_apartment_rent_(per_week)',\n",
    "    'avg._years_held',\n",
    "    'median_house_rent_(per_week)',\n",
    "    'median_house_price_(2021)',\n",
    "    'median_house_price_(2020)',\n",
    "    '%_change',\n",
    "    'traffic',\n",
    "    'public_housing_%',\n",
    "    'time_to_cbd_(public_transport)_[town_hall_st]',\n",
    "    'time_to_cbd_(driving)_[town_hall_st]'\n",
    "]\n",
    "\n",
    "suburb[median_cols] = suburb[median_cols].fillna(suburb[median_cols].median())\n",
    "\n",
    "# dropping more irrelevent columns\n",
    "suburb.drop(columns=[\"things_to_see/do\",\"postcode\"], inplace=True, errors=\"ignore\")\n",
    "suburb.sample(20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cols = [\n",
    "    \"traffic\", \"public_transport\", \"affordability_(rental)\", \"affordability_(buying)\",\n",
    "    \"nature\", \"noise\", \"family-friendliness\", \"pet_friendliness\",\n",
    "    \"safety\", \"overall_rating\"\n",
    "]\n",
    "zero_counts = {col: (suburb[col] == 0).sum() for col in cols} # checking the 0 count in certain columns, to drop the ones with 0s\n",
    "print(zero_counts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "suburb.drop(columns=cols, inplace=True, errors=\"ignore\")\n",
    "suburb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Final check-verify no missing values remain\n",
    "print(suburb.isnull().sum())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# CLEANING DOMAIN DATASET\n",
    "print(\"Domain dataset info\\n\", domain.info())\n",
    "print(\"\\n\\nDomain dataset describe\\n\", domain.describe(include='all'))\n",
    "print(\"\\n\\nDomain dataset sample rows\\n\", domain.sample(20))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# CONVERT DATE_SOLD TO PROPER FORMAT\n",
    "domain[\"date_sold\"] = pd.to_datetime(domain[\"date_sold\"], format=\"%d/%m/%y\", errors=\"coerce\")\n",
    "\n",
    "# CHECKING FOR ANY NULLS AFTER DATE CONVERSION\n",
    "invalid_dates = domain[domain[\"date_sold\"].isna()]\n",
    "print(invalid_dates)\n",
    "\n",
    "# CHECKING SPECIFIC FEATURES FOR OUTLIERS\n",
    "domain[[\"num_bath\", \"num_bed\", \"num_parking\", \"property_size\"]].describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# CHECKING THE NUMBER OF OUTLIERS\n",
    "# setting limits for number of bedrooms, bathrooms, parking and property size-anything outside these limits is a rare occasion making it an outlier\n",
    "print(\"Bath outliers:\", domain[domain['num_bath'] > 10].shape[0])\n",
    "print(\"Bed outliers:\", domain[domain['num_bed'] > 10].shape[0])\n",
    "print(\"Parking outliers:\", domain[domain['num_parking'] > 10].shape[0])\n",
    "print(\"Property size outliers:\", domain[domain['property_size'] > 5000].shape[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# DROPPING THE OUTLIERS\n",
    "domain = domain[\n",
    "    (domain['num_bath'] <= 10) &\n",
    "    (domain['num_bed'] <= 10) &\n",
    "    (domain['num_parking'] <= 10) &\n",
    "    (domain['property_size'] <= 5000)\n",
    "]\n",
    "\n",
    "domain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# CHECKING DOMAIN FOR ANY REMAINING STEPS\n",
    "domain.dtypes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# dropping irrelevent columns\n",
    "drop_cols = [\n",
    "    \"suburb_sqkm\",\n",
    "    \"suburb_lat\",\n",
    "    \"suburb_lng\",\n",
    "    \"suburb_elevation\"\n",
    "]\n",
    "\n",
    "domain.drop(columns=drop_cols, inplace=True, errors=\"ignore\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# checking the different types of properties\n",
    "for val in domain[\"type\"].unique():\n",
    "    print(val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# CONSOLIDATING TYPE COLUMN FROM DOMAIN\n",
    "# limiting the different types of properties\n",
    "type_map = {\n",
    "    'House' : 'House',\n",
    "    'Townhouse' : 'House',\n",
    "    'Semi-Detached' : 'House',\n",
    "    'Duplex' : 'House',\n",
    "    'Villa' : 'House',\n",
    "    'Terrace' : 'House',\n",
    "    'Vacant land' : 'Land',\n",
    "    'New land' : 'Land',\n",
    "    'Apartment / Unit / Flat' : 'Apartment',\n",
    "    'Studio' : 'Apartment',\n",
    "    'Block of Units' : 'Apartment',\n",
    "    'New House & Land' : 'Off the Plan House',\n",
    "    'New Apartments / Off the Plan' : 'Off the Plan Apartments',\n",
    "    'Development Site' : 'Other',\n",
    "    'Acreage / Semi-Rural' : 'Other',\n",
    "    'Rural' : 'Other'\n",
    "}\n",
    "\n",
    "domain[\"type\"] = domain[\"type\"].map(type_map)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# CHECKING AFTER CONSOLIDATING\n",
    "for val in domain[\"type\"].unique():\n",
    "    print(val)\n",
    "\n",
    "print(\"\\n\",domain.isnull().sum())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Merging Datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# MERGING DATASETS\n",
    "merged = domain.merge(suburb, on=\"suburb\", how=\"left\")\n",
    "print(merged.isnull().sum())\n",
    "merged.to_csv(\"data/merged_dataset.csv\") # saved merged dataset in /data folder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# DROPPING UNMATCHED ROWS\n",
    "merged.dropna(inplace=True)\n",
    "\n",
    "print(merged.isnull().sum())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "merged.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "merged[\"type\"].value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Initial Exploration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# UNIVARIATE ANALYSIS\n",
    "merged.hist(figsize=(40,25))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# CHECKING CORRELATION\n",
    "plt.figure(figsize=(20,15))\n",
    "sns.heatmap(merged.corr(numeric_only=True), annot=True, cmap=\"coolwarm\", fmt=\".2f\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Deep Cleaning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# FILTERING CORR VALUES WITH PRICE\n",
    "corr_matrix = merged.corr(numeric_only=True)\n",
    "\n",
    "price_corr = corr_matrix[\"price\"].sort_values(ascending=False)\n",
    "print(price_corr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# DROPPING COLUMNS BASED ON LOW CORR\n",
    "drop_cols = [\n",
    "    \"avg._years_held\",\n",
    "    \"%_change\",\n",
    "    \"public_housing_%\",\n",
    "    \"cash_rate\"\n",
    "]\n",
    "\n",
    "merged.drop(columns=drop_cols, inplace=True, errors=\"ignore\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# CHECKING MULTICOLLINEARITY\n",
    "high_corr = corr_matrix.abs() >= 0.8\n",
    "np.fill_diagonal(high_corr.values, False)\n",
    "high_corr_pairs = high_corr[high_corr].stack().reset_index()\n",
    "high_corr_pairs.columns = ['Feature 1', 'Feature 2', 'Correlation']\n",
    "print(high_corr_pairs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# FILTERING COLUMNS BASED ON THE MULTICOLLINEARITY\n",
    "to_drop = []\n",
    "for _, row in high_corr_pairs.iterrows():\n",
    "    f1, f2 = row[\"Feature 1\"], row[\"Feature 2\"]\n",
    "    if price_corr[f1] < 0.3 and price_corr[f2] < 0.3:\n",
    "        to_drop.append(f2 if price_corr[f1] >= price_corr[f2] else f1)\n",
    "        \n",
    "to_drop = list(set(to_drop))\n",
    "\n",
    "merged.drop(columns=to_drop, inplace=True, errors=\"ignore\")\n",
    "\n",
    "merged.columns, merged.dtypes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Exploratory Data Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "numerical_cols = merged.select_dtypes(include=[\"int64\", \"float64\"]).columns\n",
    "\n",
    "for col in numerical_cols:\n",
    "    fig, axes = plt.subplots(1, 2, figsize=(12, 4))\n",
    "    sns.histplot(merged[col], kde=True, ax=axes[0])\n",
    "    axes[0].set_title(f'Histogram of {col}')\n",
    "    sns.boxplot(x=merged[col], ax=axes[1])\n",
    "    axes[1].set_title(f'Boxplot of {col}')\n",
    "    plt.tight_layout()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# DUPLICATING THE MERGED DATASET\n",
    "merged_lr = merged.copy()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Feature Engineering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# LOG TRANSFORMATION\n",
    "log_transform_cols = [\n",
    "    \"price\",\n",
    "    \"property_size\",\n",
    "    \"population_(rounded)*\",\n",
    "    \"median_house_price_(2020)\",\n",
    "    \"median_house_price_(2021)\",\n",
    "    \"median_house_rent_(per_week)\",\n",
    "    \"median_apartment_price_(2020)\",\n",
    "    \"median_apartment_rent_(per_week)\"\n",
    "]\n",
    "\n",
    "for col in log_transform_cols:\n",
    "    merged_lr[col] = np.log1p(merged_lr[col])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# CAPPING OUTLIERS\n",
    "merged_lr[\"num_bath\"] = merged_lr[\"num_bath\"].clip(upper=6)\n",
    "merged_lr[\"num_bed\"] = merged_lr[\"num_bed\"].clip(upper=6)\n",
    "merged_lr[\"num_parking\"] = merged_lr[\"num_parking\"].clip(upper=6)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# CHECKING NUMERIC PLOTS AFTER TRANFORMATIONS\n",
    "numerical_cols = merged_lr.select_dtypes(include=[\"int64\", \"float64\"]).columns\n",
    "\n",
    "for col in numerical_cols:\n",
    "    fig, axes = plt.subplots(1, 2, figsize=(12, 4))\n",
    "    sns.histplot(merged_lr[col], kde=True, ax=axes[0])\n",
    "    axes[0].set_title(f'Histogram of {col}')\n",
    "    sns.boxplot(x=merged_lr[col], ax=axes[1])\n",
    "    axes[1].set_title(f'Boxplot of {col}')\n",
    "    plt.tight_layout()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "merged_lr = pd.get_dummies(merged_lr, columns=[\"type\"], drop_first=True)\n",
    "merged_lr.rename(columns={\n",
    "    \"type_House\" : \"House\",\n",
    "    \"type_Land\" : \"Land\",\n",
    "    \"type_Off the Plan Apartments\" : \"Off the Plan Apartments\",\n",
    "    \"type_Off the Plan House\" : \"Off the Plan House\",\n",
    "    \"type_Other\" : \"Other\"\n",
    "}, inplace=True)\n",
    "# cant execute this again have to runall"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "merged_lr.columns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train-Test Split & Scaling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TRAIN TEST SPLIT\n",
    "X = merged_lr.drop(columns=[\"price\", \"suburb\", \"date_sold\"])\n",
    "y = merged_lr[\"price\"]\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# SCALING\n",
    "scalar = StandardScaler()\n",
    "\n",
    "X_train_scaled = scalar.fit_transform(X_train)\n",
    "X_test_scaled = scalar.transform(X_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Baseline Linear Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# BASELINE LINEAR REGRESSION\n",
    "lr_model = LinearRegression()\n",
    "lr_model.fit(X_train_scaled, y_train)\n",
    "\n",
    "lr_y_pred = lr_model.predict(X_test_scaled)\n",
    "\n",
    "lr_rmse = np.sqrt(mean_squared_error(y_test, lr_y_pred))\n",
    "lr_mse = mean_squared_error(y_test, lr_y_pred)\n",
    "lr_mae = mean_absolute_error(y_test, lr_y_pred)\n",
    "lr_r2 = r2_score(y_test, lr_y_pred)\n",
    "\n",
    "print(\n",
    "    \"Linear Model Results:\\nRMSE = \", lr_rmse,\n",
    "    \"\\nMSE = \", lr_mse,\n",
    "    \"\\nMAE = \", lr_mae,\n",
    "    \"\\nR2 = \",lr_r2\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Residual Diagnostics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# CHECKING RESIDUALS\n",
    "lr_residuals = y_test - lr_y_pred\n",
    "\n",
    "# RESIDUALS VS FITTED\n",
    "plt.figure(figsize=(12, 12))\n",
    "sns.scatterplot(x = lr_y_pred, y = lr_residuals)\n",
    "plt.axhline(0, color = \"red\", linestyle = \"--\")\n",
    "plt.xlabel(\"Fitted Values\")\n",
    "plt.ylabel(\"Residuals\")\n",
    "plt.title(\"Residuals vs Fitted Values\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Q-Q PLOT\n",
    "plt.figure(figsize=(12,12))\n",
    "stats.probplot(lr_residuals, dist = \"norm\", plot = plt)\n",
    "plt.title(\"Q-Q Plot\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# HISTOGRAM OF RESIDUAL\n",
    "plt.figure(figsize=(12,12))\n",
    "sns.histplot(lr_residuals, kde = True)\n",
    "plt.xlabel(\"Residuals\")\n",
    "plt.title(\"Histogram of Residuals\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# STANDARDISING THE RESIDUALS\n",
    "std_residuals = lr_residuals / np.std(lr_residuals)\n",
    "\n",
    "plt.figure(figsize=(12,12))\n",
    "sns.scatterplot(x = lr_y_pred, y = std_residuals)\n",
    "plt.axhline(0, color = \"red\", linestyle = \"--\")\n",
    "plt.xlabel(\"Fitted values\")\n",
    "plt.ylabel(\"Standardised Residuals\")\n",
    "plt.title(\"Standardised Residuals vs Fitted\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Interaction Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# FEATURE INTERACTION\n",
    "# Chosen combos:\n",
    "# num_bed × property_size\n",
    "# num_bath × median_house_price_(2021)\n",
    "# suburb_median_income × median_house_price_(2021)\n",
    "# num_parking × suburb_median_income\n",
    "\n",
    "X_train_interaction = X_train.copy()\n",
    "X_test_interaction = X_test.copy()\n",
    "\n",
    "X_train_interaction[\"bed * size\"] = X_train_interaction[\"num_bed\"] * X_train_interaction[\"property_size\"]\n",
    "X_test_interaction[\"bed * size\"] = X_test_interaction[\"num_bed\"] * X_test_interaction[\"property_size\"]\n",
    "\n",
    "X_train_interaction[\"bath * price\"] = X_train_interaction[\"num_bath\"] * X_train_interaction[\"median_house_price_(2021)\"]\n",
    "X_test_interaction[\"bath * price\"] = X_test_interaction[\"num_bath\"] * X_test_interaction[\"median_house_price_(2021)\"]\n",
    "\n",
    "X_train_interaction[\"income * price\"] = X_train_interaction[\"suburb_median_income\"] * X_train_interaction[\"median_house_price_(2021)\"]\n",
    "X_test_interaction[\"income * price\"] = X_test_interaction[\"suburb_median_income\"] * X_test_interaction[\"median_house_price_(2021)\"]\n",
    "\n",
    "X_train_interaction[\"parking * income\"] = X_train_interaction[\"num_parking\"] * X_train_interaction[\"suburb_median_income\"]\n",
    "X_test_interaction[\"parking * income\"] = X_test_interaction[\"num_parking\"] * X_test_interaction[\"suburb_median_income\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# RETRAINING MODEL WITH INTERACTION TERMS\n",
    "X_train_interaction_scaled = scalar.fit_transform(X_train_interaction)\n",
    "X_test_interaction_scaled = scalar.transform(X_test_interaction)\n",
    "\n",
    "lr_interaction_model = LinearRegression()\n",
    "lr_interaction_model.fit(X_train_interaction_scaled, y_train)\n",
    "\n",
    "lr_y_pred_interaction = lr_interaction_model.predict(X_test_interaction_scaled)\n",
    "\n",
    "lr_rmse_interaction = np.sqrt(mean_squared_error(y_test, lr_y_pred_interaction))\n",
    "lr_mse_interaction = mean_squared_error(y_test, lr_y_pred_interaction)\n",
    "lr_mae_interaction = mean_absolute_error(y_test, lr_y_pred_interaction)\n",
    "lr_r2_interaction = r2_score(y_test, lr_y_pred_interaction)\n",
    "\n",
    "print(\n",
    "    \"Linear Model Results with Interaction terms:\\nRMSE = \", lr_rmse_interaction,\n",
    "      \"\\nMSE = \", lr_mse_interaction,\n",
    "      \"\\nMAE = \", lr_mae_interaction,\n",
    "      \"\\nR2 = \",lr_r2_interaction\n",
    "      )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Regularisation Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# REGULARISATION\n",
    "# RIDGE\n",
    "ridge_alphas = np.logspace(-4, 4, 100)\n",
    "ridge_cv = RidgeCV(alphas=ridge_alphas, cv=5)\n",
    "ridge_cv.fit(X_train_interaction_scaled, y_train)\n",
    "\n",
    "ridge_y_pred = ridge_cv.predict(X_test_interaction_scaled)\n",
    "\n",
    "ridge_rmse = np.sqrt(mean_squared_error(y_test, ridge_y_pred))\n",
    "ridge_mse = mean_squared_error(y_test, ridge_y_pred)\n",
    "ridge_mae = mean_absolute_error(y_test, ridge_y_pred)\n",
    "ridge_r2 = r2_score(y_test, ridge_y_pred)\n",
    "\n",
    "print(\n",
    "    \"Ridge Regression Results:\\nRMSE = \",ridge_rmse,\n",
    "    \"\\nMSE = \", ridge_mse,\n",
    "    \"\\nMAE = \", ridge_mae,\n",
    "    \"\\nR2 = \",ridge_r2\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# LASSO\n",
    "lasso_alphas = np.logspace(-4, 4, 100)\n",
    "lasso_cv = LassoCV(alphas=lasso_alphas, cv=5)\n",
    "lasso_cv.fit(X_train_interaction_scaled, y_train)\n",
    "\n",
    "lasso_y_pred = lasso_cv.predict(X_test_interaction_scaled)\n",
    "\n",
    "lasso_rmse = np.sqrt(mean_squared_error(y_test, lasso_y_pred))\n",
    "lasso_mse = mean_squared_error(y_test, lasso_y_pred)\n",
    "lasso_mae = mean_absolute_error(y_test, lasso_y_pred)\n",
    "lasso_r2 = r2_score(y_test, lasso_y_pred)\n",
    "\n",
    "print(\n",
    "    \"Lasso Regression Results:\\nRMSE = \",lasso_rmse,\n",
    "    \"\\nMSE = \", lasso_mse,\n",
    "    \"\\nMAE = \", lasso_mae,\n",
    "    \"\\nR2 = \",lasso_r2\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ELASTICNET\n",
    "elasticnet_alphas = np.logspace(-4, 4, 100)\n",
    "elasticnet_l1_ratios = np.linspace(0.1, 0.9, 9)\n",
    "\n",
    "elasticnet_cv = ElasticNetCV(alphas=elasticnet_alphas, l1_ratio=elasticnet_l1_ratios, cv=5)\n",
    "elasticnet_cv.fit(X_train_interaction_scaled, y_train)\n",
    "\n",
    "elasticnet_y_pred = elasticnet_cv.predict(X_test_interaction_scaled)\n",
    "\n",
    "elasticnet_rmse = np.sqrt(mean_squared_error(y_test, elasticnet_y_pred))\n",
    "elasticnet_mse = mean_squared_error(y_test, elasticnet_y_pred)\n",
    "elasticnet_mae = mean_absolute_error(y_test, elasticnet_y_pred)\n",
    "elasticnet_r2 = r2_score(y_test, elasticnet_y_pred)\n",
    "\n",
    "print(\n",
    "    \"ElastiNet Regression Results:\\nRMSE = \",elasticnet_rmse,\n",
    "    \"\\nMSE = \", elasticnet_mse,\n",
    "    \"\\nMAE = \", elasticnet_mae,\n",
    "    \"\\nR2 = \",elasticnet_r2\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Decision Tree Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# BASELINE DECISION TREE\n",
    "dt_model = DecisionTreeRegressor(random_state=42)\n",
    "dt_model.fit(X_train_interaction, y_train)\n",
    "\n",
    "dt_y_pred = dt_model.predict(X_test_interaction)\n",
    "\n",
    "dt_rmse = np.sqrt(mean_squared_error(y_test, dt_y_pred))\n",
    "dt_mse = mean_squared_error(y_test, dt_y_pred)\n",
    "dt_mae = mean_absolute_error(y_test, dt_y_pred)\n",
    "dt_r2 = r2_score(y_test, dt_y_pred)\n",
    "\n",
    "print(\n",
    "    \"Decision Tree Regression Results:\\nRMSE = \",dt_rmse,\n",
    "    \"\\nMSE = \", dt_mse,\n",
    "    \"\\nMAE = \", dt_mae,\n",
    "    \"\\nR2 = \",dt_r2\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# HYPERPARAMETER DECISION TREE\n",
    "dt_params = {\n",
    "    \"max_depth\" : [5, 10, 15, 20, None],\n",
    "    \"min_samples_split\" : [2, 5, 10],\n",
    "    \"min_samples_leaf\" : [1, 2, 4],\n",
    "    \"max_features\" : [\"auto\", \"sqrt\", \"log2\", None]\n",
    "}\n",
    "\n",
    "dt_grid = GridSearchCV(\n",
    "    DecisionTreeRegressor(random_state=42),\n",
    "    param_grid=dt_params,\n",
    "    cv=5,\n",
    "    scoring=\"neg_mean_squared_error\",\n",
    "    n_jobs=1\n",
    ")\n",
    "\n",
    "dt_grid.fit(X_train_interaction, y_train)\n",
    "best_dt = dt_grid.best_estimator_\n",
    "\n",
    "dt_y_pred_best = best_dt.predict(X_test_interaction)\n",
    "\n",
    "dt_rmse_best = np.sqrt(mean_squared_error(y_test, dt_y_pred_best))\n",
    "dt_mse_best = mean_squared_error(y_test, dt_y_pred_best)\n",
    "dt_mae_best = mean_absolute_error(y_test, dt_y_pred_best)\n",
    "dt_r2_best = r2_score(y_test, dt_y_pred_best)\n",
    "\n",
    "print(\n",
    "    \"Tuned Decision Tree Regression Results:\\nRMSE = \",dt_rmse_best,\n",
    "    \"\\nMSE = \", dt_mse_best,\n",
    "    \"\\nMAE = \", dt_mae_best,\n",
    "    \"\\nR2 = \",dt_r2_best\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# RANDOM FORREST\n",
    "rf_params = {\n",
    "    \"n_estimators\" : [100, 200],\n",
    "    \"max_depth\": [10, 20, None],\n",
    "    \"min_samples_split\" : [2, 5],\n",
    "    \"min_samples_leaf\" : [1, 2],\n",
    "    \"max_features\" : [\"sqrt\", \"log2\"]\n",
    "}\n",
    "\n",
    "rf_grid = GridSearchCV(\n",
    "    RandomForestRegressor(random_state=42),\n",
    "    param_grid=rf_params,\n",
    "    cv=5,\n",
    "    scoring=\"neg_mean_squared_error\",\n",
    "    n_jobs=1\n",
    ")\n",
    "\n",
    "rf_grid.fit(X_train_interaction, y_train)\n",
    "best_rf = rf_grid.best_estimator_\n",
    "\n",
    "rf_y_pred_best = best_rf.predict(X_test_interaction)\n",
    "\n",
    "rf_rmse_best = np.sqrt(mean_squared_error(y_test, rf_y_pred_best))\n",
    "rf_mse_best = mean_squared_error(y_test, rf_y_pred_best)\n",
    "rf_mae_best = mean_absolute_error(y_test, rf_y_pred_best)\n",
    "rf_r2_best = r2_score(y_test, rf_y_pred_best)\n",
    "\n",
    "print(\n",
    "    \"Random Forrest Regression Results:\\nRMSE = \",rf_rmse_best,\n",
    "    \"\\nMSE = \", rf_mse_best,\n",
    "    \"\\nMAE = \", rf_mae_best,\n",
    "    \"\\nR2 = \",rf_r2_best\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model Comparison Plots"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_names = [\n",
    "    \"Linear (Baseline)\",\n",
    "    \"Linear (Interaction)\",\n",
    "    \"Ridge\",\n",
    "    \"Lasso\",\n",
    "    \"Elasticnet\",\n",
    "    \"Decision Tree\",\n",
    "    \"Decision Tree (Tuned)\",\n",
    "    \"Random Forrest\"\n",
    "]\n",
    "\n",
    "r2_scores = [lr_r2, lr_r2_interaction, ridge_r2, lasso_r2, elasticnet_r2, dt_r2, dt_r2_best, rf_r2_best]\n",
    "rmse_scores = [lr_rmse, lr_rmse_interaction, ridge_rmse, lasso_rmse, elasticnet_rmse, dt_rmse, dt_rmse_best, rf_rmse_best]\n",
    "mse_scores = [lr_mse, lr_mse_interaction, ridge_mse, lasso_mse, elasticnet_mse, dt_mse, dt_mse_best, rf_mse_best]\n",
    "mae_scores = [lr_mae, lr_mae_interaction, ridge_mae, lasso_mae, elasticnet_mae, dt_mae, dt_mae_best, rf_mae_best]\n",
    "\n",
    "metrics = {\n",
    "    \"R2 Score\" : {\"values\": r2_scores, \"color\": \"green\", \"ylabel\": \"R2 Score\"},\n",
    "    \"RMSE\" : {\"values\": rmse_scores, \"color\": \"blue\", \"ylabel\": \"RMSE\"},\n",
    "    \"MSE\" : {\"values\": mse_scores, \"color\": \"black\", \"ylabel\": \"MSE\"},\n",
    "    \"MAE\" : {\"values\": mae_scores, \"color\": \"red\", \"ylabel\": \"MAE\"},\n",
    "}\n",
    "\n",
    "for title, config in metrics.items():\n",
    "    plt.figure(figsize=(15, 8))\n",
    "    plt.plot(model_names, config[\"values\"], marker='o', color=config[\"color\"])\n",
    "    for i, v in enumerate(config[\"values\"]):\n",
    "        plt.text(i, v + 0.005, f\"{v:.3f}\", ha='center', color=config[\"color\"])\n",
    "    plt.title(f\"{title} Comparison\")\n",
    "    plt.xlabel(\"Model\")\n",
    "    plt.ylabel(config[\"ylabel\"])\n",
    "    plt.xticks(rotation=45)\n",
    "    plt.grid(True, linestyle='--', linewidth=0.5)\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
